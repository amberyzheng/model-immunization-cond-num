GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/zheng709/miniconda3/envs/model-immu/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

/home/zheng709/miniconda3/envs/model-immu/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]

  | Name              | Type                   | Params | Mode
---------------------------------------------------------------------
0 | feature_extractor | LinearFeatureExtractor | 6.3 K  | train
  | other params      | n/a                    | 79     | n/a
---------------------------------------------------------------------
6.4 K     Trainable params
0         Non-trainable params
6.4 K     Total params
0.026     Total estimated model params size (MB)
2         Modules in train mode
0         Modules in eval mode
/home/zheng709/miniconda3/envs/model-immu/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.70it/s, v_num=58nm, train_loss_step=1.49e+6, train_loss_epoch=1.49e+6]
/home/zheng709/miniconda3/envs/model-immu/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('r1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/zheng709/miniconda3/envs/model-immu/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('r2', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/zheng709/miniconda3/envs/model-immu/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('cond_S1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/zheng709/miniconda3/envs/model-immu/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('cond_S2', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/zheng709/miniconda3/envs/model-immu/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/zheng709/miniconda3/envs/model-immu/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('reconstruction_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/zheng709/miniconda3/envs/model-immu/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('regularization_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
`Trainer.fit` stopped: `max_epochs=10` reached.
X1 condition: 1795161651.0645268, A1 condition: 394404313.74806696
X2 condition: 2009611173.1950781, A2 condition: 531779644.03056484
Immunization Gap: 1.2044300378686656
